{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 그래디언트 소실과 폭주 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1 글로럿과 He 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬은 기본적으로 글로럿 초기화를 사용한다고 한다. 만약 바꾸고 싶으면\n",
    "```{.python}\n",
    "keras.layers.Dense(10, activation = 'relu', kernel_initializer = 'he_normal')\n",
    "```\n",
    "로 해주면 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 수렴하지 않는 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 렐루 층을 만들고 적용하려는 층 뒤에 추가하면 된다.\n",
    "\n",
    "``` {.python}\n",
    "model = keras.models.Sequential([\n",
    "    [...]\n",
    "    keras.layers.Dense(10, kernel_initializer = 'he_normal',\n",
    "    keras.layers.LeakyReLU(alpha = 0.2),\n",
    "    [...]\n",
    "    ])\n",
    "```    \n",
    "\n",
    "얘처럼 alpha = 0.2로 두면 PReLU가 된다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화함수를 바꾸고 커널초기화를 르쿤으로 바꾸면 된다고 함\n",
    "\n",
    "```{.python}layer = keras.layer.Dense(10, activation = 'selu', kernel_initializer = 'lecun_normal')```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 Batch Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 layers로 배치 정규화 층을 만들어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pydot\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x1e169d27a30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 케라스를 사용한 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#model 불러오기\n",
    "```{.python}\n",
    "model_a = keras.models.load_model(\"my_keras_model.h5\")\n",
    "```\n",
    "\n",
    "#model 불러서 넣기\n",
    "```{.python}\n",
    "model_b_on_a = keras.models.Sequential(model_a.layers[:-1])\n",
    "model_b_on_a.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_b_on_a 를 훈련하면 model_a도 영향을 받는다고 한다. 그러므로 층을 재사용 하기 전에 model_a를 복제해야한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#clone만들기\n",
    "```{.python}\n",
    "model_a_clone = keras.models.clone_model(model_a)\n",
    "model_a_clone.set_weights(model_a.get_weights())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9)\n",
    "```\n",
    "\n",
    "모멘텀 파라미터인 베타가 추가되긴 하지만, 보통 0.9에서 좋은 성능을 낸다고 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2 네스테로프 가속 경사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "optimizer = keras.optimizers.SGD(lr = 0.01, momentum = 0.9, nesterov = True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 신경망에서 사용하면 안된다고 한다. 학습률이 너무 감소되어 global minima에 도달하기 전에 멈춰버리기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.4 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "optimizer = keras.optimizers.RMSprop(lr = 0.001, rho = 0.9)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.5 Adam과 변종들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "```\n",
    "beta1은 모멘텀 감쇠 파라미터, beta2는 스케일 감쇠 파라미터라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.6 학습률 스케쥴링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 거듭제곱 기반 스케쥴링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 0.01, decay = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지수 기반 스케쥴링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch) : \n",
    "    return 0.01 * 0.1 ** (epoch/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s) : \n",
    "    def exponential_decay_fn(epoch) : \n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0 = 0.01, s = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- call back 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-342fc14c6e08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexponential_decay_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(x_train_scaled, y_train, callbacks = [lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능 기반 스케쥴링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch) : \n",
    "    if epoch <5 : \n",
    "        return 0.01\n",
    "    elif epoch <15 : \n",
    "        return 0.005\n",
    "    else :\n",
    "        return 0.001\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-34f537feb039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExponentialDecay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "s = 20 * len(x_train) // 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 L1 & L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l2\n",
    "layer = keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal'\n",
    "                          , kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "#l2 & l1\n",
    "layer = keras.layers.Dense(100, activation = 'elu', kernel_initializer = 'he_normal'\n",
    "                          , kernel_regularizer = keras.regularizers.l1_l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 모든 은닉층에 동일한 활성화함수, 동일한 초기화전략, 동일한 규제를 사용하기 때문에 하나로 만들어주는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 코드 리팩터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, \n",
    "                          activation = 'elu'\n",
    "                          ,kernel_initializer = 'he_normal'\n",
    "                          , kernel_regularizer = keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28])\n",
    "    ,RegularizedDense(300)\n",
    "    ,RegularizedDense(100)\n",
    "    \n",
    "    #출력층 생성\n",
    "    ,RegularizedDense(10, activation = 'softmax'\n",
    "                     ,kernel_initializer = 'glorot_uniform')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> partial함수 : 하나이상의 인수가 채워진 함수의 새 버전을 만들기 위해 사용된다. 저기선 나머지는 고정돼 있고, units만 새로이 바꿔주는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드롭아웃으로 훈련된 뉴런은 이웃한 뉴런에 의존할 수 없기 때문에 최적의 결과를 내기 위해 좋은 성능을 가지는 방향으로 학습힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    #입력층\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    \n",
    "    #은닉층 1\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    \n",
    "    #은닉층2\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "    \n",
    "     #출력층\n",
    "    keras.layers.Dense(100, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3 Monte-Carlo Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "#데이터 get\n",
    "(x_train_full, y_train_full), (x_holdout, y_holdout) = fashion_mnist.load_data()\n",
    "\n",
    "#훈련 데이터 - 검증 데이터 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Type          Data/Info\n",
      "------------------------------------------\n",
      "fashion_mnist      module        <module 'tensorflow.keras<...>hion_mnist\\\\__init__.py'>\n",
      "keras              module        <module 'tensorflow.keras<...>low\\\\keras\\\\__init__.py'>\n",
      "model              Sequential    <tensorflow.python.keras.<...>ct at 0x000001E160A3C3A0>\n",
      "np                 module        <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "pd                 module        <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pydot              module        <module 'pydot' from 'C:\\<...>site-packages\\\\pydot.py'>\n",
      "tf                 module        <module 'tensorflow' from<...>tensorflow\\\\__init__.py'>\n",
      "train_test_split   function      <function train_test_split at 0x000001E16ADFA940>\n",
      "x_holdout          ndarray       10000x28x28: 7840000 elems, type `uint8`, 7840000 bytes (7.476806640625 Mb)\n",
      "x_train            ndarray       42000x28x28: 32928000 elems, type `uint8`, 32928000 bytes (31.402587890625 Mb)\n",
      "x_train_full       ndarray       60000x28x28: 47040000 elems, type `uint8`, 47040000 bytes (44.86083984375 Mb)\n",
      "x_valid            ndarray       18000x28x28: 14112000 elems, type `uint8`, 14112000 bytes (13.458251953125 Mb)\n",
      "y_holdout          ndarray       10000: 10000 elems, type `uint8`, 10000 bytes\n",
      "y_train            ndarray       42000: 42000 elems, type `uint8`, 42000 bytes\n",
      "y_train_full       ndarray       60000: 60000 elems, type `uint8`, 60000 bytes\n",
      "y_valid            ndarray       18000: 18000 elems, type `uint8`, 18000 bytes\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 1ms/step - loss: 0.3734 - accuracy: 0.8753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3733670115470886, 0.875333309173584]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------기본 모델 구축\n",
    "#모델 구축 \n",
    "model_base = keras.models.Sequential([\n",
    "    #입력층\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    #은닉층1\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    #은닉층2\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    #출력층\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#모델 컴파일\n",
    "model_base.compile(loss = 'sparse_categorical_crossentropy', \n",
    "             optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), #Adam\n",
    "              metrics = ['accuracy'] #훈련과 평가 시에 정확도를 측정하기 위해 accuracy를 사용!\n",
    "             )\n",
    "\n",
    "#early_stopping 지정\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)\n",
    "\n",
    "#모델 적합\n",
    "history = model_base.fit(x = x_train, y = y_train, epochs = 100 #30번 반복?\n",
    "                   , validation_data = (x_valid, y_valid) #이렇게 검증 셋을 지정해줄 수 있음\n",
    "                   ,verbose = 0, callbacks = [early_stopping_cb]) # 더이상 결과가 창에 뜨지 않는다!\n",
    "\n",
    "#validation에 대한 성능 검증\n",
    "model_base.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model_base.predict(x_holdout[:1]),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 모델은 새로운 데이터가 거의 앵클부츠(91%로의 확률로)에 해당한다고 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "- Monte Carlo Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6076 - accuracy: 0.8021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6076083183288574, 0.8020555377006531]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Monte-Carlo 정의\n",
    "class MCDropout(keras.layers.Dropout) : \n",
    "    def call(self, inputs) : \n",
    "         return super().call(inputs, training = True)\n",
    "        \n",
    "        \n",
    "#모델 구축 \n",
    "model_mc = keras.models.Sequential([\n",
    "    #입력층\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    MCDropout(rate = 0.2),\n",
    "    #은닉층1\n",
    "    keras.layers.Dense(300, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    MCDropout(rate = 0.2),\n",
    "    #은닉층2\n",
    "    keras.layers.Dense(100, activation = 'elu', kernel_initializer='he_normal'),\n",
    "    MCDropout(rate = 0.2),\n",
    "    #출력층\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "#모델 컴파일\n",
    "model_mc.compile(loss = 'sparse_categorical_crossentropy', \n",
    "             optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), #Adam\n",
    "              metrics = ['accuracy'] #훈련과 평가 시에 정확도를 측정하기 위해 accuracy를 사용!\n",
    "             )\n",
    "\n",
    "#early_stopping 지정\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)\n",
    "\n",
    "#모델 적합\n",
    "history = model_mc.fit(x = x_train, y = y_train, epochs = 100 #30번 반복?\n",
    "                   , validation_data = (x_valid, y_valid) #이렇게 검증 셋을 지정해줄 수 있음\n",
    "                   ,verbose = 0, callbacks = [early_stopping_cb]) # 더이상 결과가 창에 뜨지 않는다!\n",
    "\n",
    "#validation에 대한 성능 검증\n",
    "model_mc.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model_mc.predict(x_holdout[:1]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
