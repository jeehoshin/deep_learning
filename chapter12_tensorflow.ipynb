{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : 핸즈온 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter12 텐서플로를 사용한 사용자 정의 모델과 훈련 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 텐서플로 훑어보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 API가 존재하여 이에 대해 자세히 살펴보아야 한다고 한다. \n",
    "\n",
    "1) 저수준 딥러닝 API\n",
    "- ```tf.nn```\n",
    "- ```tf.losses```\n",
    "- ```tf.metrics```\n",
    "- ```tf.optimizers```\n",
    "- ```tf.train```\n",
    "- ```tf.initializers```\n",
    "\n",
    "2) 자동미분\n",
    "- ```tf.GradientTape```\n",
    "- ```tf.gradients()```\n",
    "\n",
    "3) 입출력과 전처리\n",
    "- ```tf.data```\n",
    "- ```tf.feature_column```\n",
    "- ```tf.audio```\n",
    "- ```tf.image```\n",
    "- ```tf.io```\n",
    "- ```tf.queue```\n",
    "\n",
    "4) 텐서보드 시각화\n",
    "- ```tf.summary```\n",
    "\n",
    "5) 배포와 최적화\n",
    "- ```tf.distribute```\n",
    "- ```tf.saved_model```\n",
    "- ```tf.autograph```\n",
    "- ```tf.graph_util```\n",
    "- ```tf.lite```\n",
    "- ```tf.quantizaion```\n",
    "- ```tf.tpu```\n",
    "- ```tf.xla```\n",
    "\n",
    "6) 특수한 데이터 구조\n",
    "- ```tf.lookup```\n",
    "- ```tf.nest```\n",
    "- ```tf.ragged```\n",
    "- ```tf.sets```\n",
    "- ```tf.sparse```\n",
    "- ```tf.strings```\n",
    "\n",
    "7) 선형대수와 신호처리를 포함한 수학연산\n",
    "- ```tf.math```\n",
    "- ```tf.linalg```\n",
    "- ```tf.signal```\n",
    "- ```tf.random```\n",
    "- ```tf.bitwise```\n",
    "\n",
    "8) 그 외 \n",
    "- ```tf.compat```\n",
    "- ```tf.config```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 넘파이처럼 텐서플로 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 텐서와 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.constant()  \n",
    "텐서를 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하나의 list가 행이 되는 것 같다.\n",
    "t = tf.constant([[1.,2.,3.], [4.,5.,6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다양한 연산 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 행과 1열부터 마지막 열까지만 뽑아라\n",
    "t[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.newaxis는 원래 차원을 변경해주는 것. 근데 여기서는 적용이 안 되는 듯 싶다.\n",
    "t[...,1,tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[11. 12. 13.]\n",
      " [14. 15. 16.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(t + 10)\n",
    "#근데 위 식은 아래 식과 같다.\n",
    "print(tf.add(t,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단순히 해당 원소의 제곱값을 찾아준다.\n",
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[14. 32.]\n",
      " [32. 77.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#파이썬 3.5부터 행렬곱은 @으로 연산한다고 한다.\n",
    "print(t @ tf.transpose(t))\n",
    "\n",
    "#이는 tf.matmul을 사용하는 것과 같다.\n",
    "print(tf.matmul(t, tf.transpose(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 넘파이와의 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```tf.add``` ```tf.multiply``` ```tf.square``` ```tf.exp``` ```tf.sqrt``` ```tf.reshape``` ```tf.squeeze``` ```tf.tile```등은 넘파이 연산과 같음\n",
    "\n",
    "> ```tf.reduce_mean``` = ```np.mean``` // ```tf.reduce_sum```=```np.sum``` // ```tf.reduce_max```=```np.max``` // ```tf.math.log```=```np.log``` 임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2 텐서와 넘파이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서와 넘파이는 함께 사용하기 편리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 5.], shape=(3,), dtype=float64)\n",
      "<dtype: 'float64'>\n",
      "\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "float32\n",
      "\n",
      "\n",
      "tf.Tensor([ 4. 16. 25.], shape=(3,), dtype=float64)\n",
      "<dtype: 'float64'>\n",
      "\n",
      "\n",
      "[[ 1.  4.  9.]\n",
      " [16. 25. 36.]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a = np.array([2., 4.,5.])\n",
    "\n",
    "#넘파이 행렬을 텐서로 만들어준다.\n",
    "print(tf.constant(a))\n",
    "print(tf.constant(a).dtype)\n",
    "print('\\n')\n",
    "\n",
    "#텐서를 넘파이화 해준다.\n",
    "print(t.numpy()) #또는 np.array(t)\n",
    "print(t.numpy().dtype)\n",
    "print('\\n')\n",
    "\n",
    "#텐서 연산에 넘파이 행렬을 넣어줌\n",
    "print(tf.square(a))\n",
    "print(tf.square(a).dtype)\n",
    "print('\\n')\n",
    "\n",
    "#넘파이 연산에 텐서를 넣어줌\n",
    "print(np.square(t))\n",
    "print(np.square(t).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 타입변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타입변환은 성능을 크게 감소시킬 수 있다(...!!!!)\n",
    "- 텐서플로는 이를 방지하기 위해 어떠한 타입 변환도 자동으로 수행하지 않는다.\n",
    "- 실수 텐서와 정수 텐서를 더할 수도 없고, 32비트 실수와 64비트 실수도 더할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-1026a7e18cf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1484\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    469\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4d5d00169e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1484\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    469\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40., dtype = tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40., dtype = tf.float32) #이렇게 하면 진행이 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.cast()  \n",
    "타입 변환을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype = tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32) #64비트를 32비트로 바꾸어 연산을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.4 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Tensor는 변경이 불가능한 객체이다.  \n",
    "따라서, 이와 같은 일반적인 텐서로는 역전파로 변경되어야 하는 신경망의 가중치를 구할 수 없다.  \n",
    "또한 시간에 따라 변경되어야 하는 모멘텀 옵티마이저의 과거 그레디언트 등을 변경할 수도 없다.  \n",
    "따라서 tf.Variable이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assign 메서드  \n",
    "해당 메서드를 통해 변수를 증가시키거나 감소시킬 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 4., 84., 12.],\n",
       "       [16., 20., 24.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v의 값을 두배로 하여 v를 바꾼다.\n",
    "#이 방식은 조금 조심해야겠는게, 여러 번 실행을 하면 계속해서 바뀌게 된다.\n",
    "v.assign(2 * v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 4., 42., 12.],\n",
       "       [16., 20., 24.]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(0,1)의 값을 42로 바꿈\n",
    "v[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 4., 42.,  0.],\n",
       "       [16., 20.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2열을 [0,1]로 바꾼다.\n",
    "v[:,2].assign([0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [ 16.,  20., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원하는 위치의 값을 따로 바꿀 수 있다.\n",
    "#이 메서드의 경우 (0,0)를 100으로 바꾸고, (1,2)를 200으로 바꾼다.\n",
    "v.scatter_nd_update(indices = [[0,0], [1,2]], updates= [100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.5 다른 데이터 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 희소 텐서(tf.SparseTensor)  \n",
    ": 대부분 0으로 채워진 텐서를 효율적으로 나타냄.\n",
    "- 텐서 배열(tf.TensorArray)  \n",
    ": 텐서의 리스트. 리스트에 포함된 모든 텐서는 크기와 데이터 타입이 동일해야 함.  \n",
    "- 래그드 텐서(tf.RaggedTensor)  \n",
    ": 리스트의 리스트를 나타낸다.  \n",
    "- 문자열 텐서(tf.string)  \n",
    ": 바이트 문자열을 나타내는데, 유니코드 문자열을 사용해 문자열 텐서를 만들면 자동으로 UTF-8로 인코딩 됨.  \n",
    "- 집합(set)  \n",
    ": 집합은 일반적인 텐서로 나타낸다. (아무래도 내부의 값을 변경할 수 없으니 집합으로 인식하는 것인가?)  \n",
    "- 큐(tf.queue)  \n",
    ": 큐는 단계별로 텐서를 저장한다. 간단한 FIFO(First In, First Out), 어떤 원소에 우선권을 주는 큐(PriorityQueue), 원소를 섞는 큐(RandomShuffleQueue), 패딩을 추가하여 크기가 다른 원소의 배치를 만드는 큐(PaddingFIFOQueue) 등이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 사용자 정의 모델과 훈련 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 사용자 정의 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀 모델을 훈련하는 데, 훈련 세트의 이상치가 있는 경우? --> 후버 손실함수를 사용하여, 일부 임계치에 대해서는 제곱을 곱하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    #실제값과 예측값의 차를 만듦\n",
    "    error = y_true - y_pred\n",
    "    \n",
    "    #1보다 작은 에러는 제곱을 적용하기 위해 논리값을 만듦.\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    \n",
    "    #loss를 계산한다.\n",
    "    squared_loss = tf.square(error)/2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> 특히 이렇게 샘플마다 하나의 손실을 담은 텐서를 반환하는 것이 좋음. 이래야 클래스 가중치나 샘플 가중치를 적용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```tf.where``` : tf.where(condition,a,b) condition이 참이면 a를 참이 아니면 b를 추출함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "#데이터 적재\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "#train test 데이터 분할\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "#train - valid 데이터 분할\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full)\n",
    "\n",
    "#표준화 작업을 거쳐주는 것 같다.\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.fit_transform(x_valid)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 객체 생성 - shape과 dtype을 이용하여 input을 구축\n",
    "#tabular 데이터에서 필요한 건 결국 칼럼의 수! 해당 obs의 개수는 계속 달라질 수 있으니까\n",
    "input_ = keras.layers.Input(shape = x_train.shape[1:])\n",
    "\n",
    "#Dense층 생성\n",
    "hidden1 = keras.layers.Dense(30, activation = 'relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1) #뒤에 이렇게 함수처럼 넣을 객체를 지정해준다.\n",
    "#함수처럼이 무슨 의미냐면 sum()처럼 만들자마자 위의 값을 함수처럼 넣는다는 의미\n",
    "\n",
    "#concatenate층으로 *두번째 은닉층과 input층을 연결해준다.*\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "#이게 왜 생긴거냐면 와이드 경로는 입력의 일부 또는 전체가 바로 출력층으로 전달! 그래서 와이드 신경망을 구축하기 위해 넣은 것이라 생각하면 된다.\n",
    "\n",
    "#출력층 만들기 - concat을 받는다는 것은 input의 값과 hidden2의 출력값을 여기서 출력해준다는 의미다.\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "#input과 output을 만들어 케라스 모델을 구축한다\n",
    "model = keras.Model(inputs = [input_], outputs = [output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = huber_fn, optimizer = 'nadam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.1716 - val_loss: 1.1219\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1663 - val_loss: 1.1425\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1635 - val_loss: 1.3806\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1591 - val_loss: 1.4932\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1547 - val_loss: 1.6400\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1521 - val_loss: 1.6977\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1486 - val_loss: 1.8556\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1467 - val_loss: 1.7740\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1441 - val_loss: 1.8068\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1428 - val_loss: 1.9615\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1412 - val_loss: 2.0270\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1395 - val_loss: 2.0824\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1390 - val_loss: 2.0830\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1375 - val_loss: 2.0366\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.1365 - val_loss: 2.2322\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1355 - val_loss: 2.1963\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.1349 - val_loss: 2.1170\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1342 - val_loss: 2.2811\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1328 - val_loss: 2.2719\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1329 - val_loss: 2.1965\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.1322 - val_loss: 2.1703\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1316 - val_loss: 2.3136\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.1315 - val_loss: 2.2458\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1302 - val_loss: 2.2209\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.1295 - val_loss: 2.2006\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1295 - val_loss: 2.2089\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1302 - val_loss: 2.2769\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.1283 - val_loss: 2.1640\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.1284 - val_loss: 2.1441\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.1278 - val_loss: 2.2431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2806be5e0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 30,\n",
    "                     validation_data = (x_valid, y_valid), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/model/my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 사용자 정의 요소를 가진 모델을 저장하고 로드하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">노란 모델을 로드할 때는 함수 이름과 실제 함수를 매핑한 딕셔너리를 전달해야 한다!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = keras.models.load_model(\"/model/my_model_with_a_custom_loss.h5\"\n",
    "                                      ,custom_objects = {\"huber_fn\" : huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다른 기준이 필요할 경우?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold = 1.0) : \n",
    "    def huber_fn(y_true, y_pred) : \n",
    "        \n",
    "        #실제값과 예측값의 차를 만듦.\n",
    "        error = y_true - y_pred\n",
    "    \n",
    "        #1보다 작은 에러는 제곱을 적용하기 위해 논리값을 만듦.\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "    \n",
    "        #loss를 계산한다.\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = treshold * tf.abs(error) - threshold**2 / 2\n",
    "\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이젠 treshold에 따라서 가변적인 손실함수를 구축한다.\n",
    "model_loaded.compile(loss = create_huber(2.0), optimizer = 'nadam')\n",
    "model_loaded.save(\"/model/my_model_with_a_custom_loss_ver1_0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제는 모델을 저장할 때 이 treshhold 값은 저장되지 않는다. 따라서 모델을 불러올때 treshold 값을 지정해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded2 = keras.models.load_model(\"/model/my_model_with_a_custom_loss_ver1_0.h5\"\n",
    "                                       ,custom_objects = {'huber_fn' : create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class 상속으로 문제를 해결할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 우선 기본적으로 keras.losses.Loss는 아래와 같이 생겼다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss base class.\n",
    "\n",
    "To be implemented by subclasses:\n",
    "* `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`.\n",
    "\n",
    "Example subclass implementation:\n",
    "\n",
    "```python\n",
    "class MeanSquaredError(Loss):\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor_v2(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)\n",
    "```\n",
    "\n",
    "When used with `tf.distribute.Strategy`, outside of built-in training loops\n",
    "such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction\n",
    "types, and reduce losses explicitly in your training loop. Using 'AUTO' or\n",
    "'SUM_OVER_BATCH_SIZE' will raise an error.\n",
    "\n",
    "Please see this custom training [tutorial](\n",
    "  https://www.tensorflow.org/tutorials/distribute/custom_training) for more\n",
    "details on this.\n",
    "\n",
    "You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n",
    "```python\n",
    "with strategy.scope():\n",
    "  loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
    "      reduction=tf.keras.losses.Reduction.NONE)\n",
    "  ....\n",
    "  loss = (tf.reduce_sum(loss_obj(labels, predictions)) *\n",
    "          (1. / global_batch_size))\n",
    "```\n",
    "Init docstring:\n",
    "Initializes `Loss` class.\n",
    "\n",
    "Args:\n",
    "  reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "    loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "    option will be determined by the usage context. For almost all cases\n",
    "    this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
    "    `tf.distribute.Strategy`, outside of built-in training loops such as\n",
    "    `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
    "    will raise an error. Please see this custom training [tutorial](\n",
    "      https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
    "    for more details.\n",
    "  name: Optional name for the op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss) : \n",
    "    \n",
    "    #기본적인 하이퍼파라미터를 **kwargs로 받은 매개변수 값을 부모 클래스의 생성자에게 전달한다.\n",
    "    #treshold의 초기값은 1.0으로 설정한다\n",
    "    def __init__(self, threshold = 1.0, **kwargs) : \n",
    "        self.threshold = threshold #treshold가 들어오면 이 값을 설정하면 됨.\n",
    "        super().__init__(**kwargs) #나머지 항목들은 필요하다면 넣어준다.\n",
    "        \n",
    "    #call() 메서드는 레이블과 예측을 받고 샘플의 손실을 계산하여 반환한다.    \n",
    "    def call(self, y_true, y_pred) : \n",
    "        error = y_true - y_pred\n",
    "    \n",
    "        #1보다 작은 에러는 제곱을 적용하기 위해 논리값을 만듦.\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "    \n",
    "        #loss를 계산한다.\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = treshold * tf.abs(error) - threshold**2 / 2\n",
    "\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    #하이퍼파리미터 이름과 같이 매핑된 딕셔너리를 반환한다.\n",
    "    def get_config(self) : \n",
    "        base_config = super().get_config() #부모 클래스의 get_config를 호출하고 \n",
    "        return {**base_config, \"treshold\" : self.threshold} #반환된 딕셔너리에 새로운 하이퍼파라미터를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_config() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-3537cca03348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_config() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "keras.losses.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
